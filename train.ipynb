{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, InputLayer, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, normalize\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from utils.data_utils import mod_df, drop_features, inverse_mod_X, inverse_mod_y, apply_savgol_filter, apply_median_filter, apply_maximum_filter, apply_is_zero, apply_standard_scale\n",
    "from utils.visualize_utils import plot_history, evaluate_model\n",
    "from my_model import create_model_1, create_model_1_1, create_model_1_2, create_model_1_3, create_model_1_4, create_model_2, create_model_2_1, create_model_2_2, create_model_3\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join('data', 'unionTrain.csv'))\n",
    "test_df = pd.read_csv(os.path.join('data', 'unionTest.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mod_df(train_df)\n",
    "X_test, y_test = mod_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    result = df.copy()\n",
    "    \n",
    "    # select & drop features\n",
    "    result = result.drop(['id', 'timestep'], axis=1)\n",
    "    # result = drop_features(result,[0,1,2,5,15,16,17,18,21,20,23,24])\n",
    "    # result = drop_features(result, [15, 16, 17, 18, 20, 21, 23, 24])\n",
    "    # result = result[['1Y','2Y','4Y','5Y','6Y','7X','8Y','11Y','12Y','13Y','14Y','20X','22Y','23X','24X','24Y']] # ts = 0.07\n",
    "    result = result[['1Y', '2Y', '4X', '4Y', '5Y', '6Y', '7X', '8Y', '10X', '11Y', '12Y', '13Y', '14X', '14Y', '20X', '21Y', '22Y', '23X', '24X', '24Y']] # ts = 0.04\n",
    "    # result = result[['1Y', '2Y', '3Y', '4X', '4Y', '5Y', '6Y', '7X', '7Y', '8Y', '10X', '11Y', '12X', '12Y', '13Y', '14X', '14Y', '16Y', '20X', '21Y', '22X', '22Y', '23X', '23Y', '24X', '24Y']] # ts = 0.001\n",
    "    DEFAULT_FEATURE = result.columns.to_list()\n",
    "    \n",
    "    # add features\n",
    "    # result['test_0X'] = result['0X']\n",
    "    FEATURE_COLUMNS = result.columns.to_list()\n",
    "    for col in tqdm(FEATURE_COLUMNS):\n",
    "        feature = result[col]\n",
    "        feature = feature.to_numpy()\n",
    "        # result[f'{col}_savgol_1'] = apply_savgol_filter(feature, window_size=21, polynomial=1)\n",
    "        # result[f'{col}_savgol_7'] = apply_savgol_filter(feature, window_size=21, polynomial=2)\n",
    "        # result[f'{col}_savgol_2'] = apply_savgol_filter(feature, window_size=21, polynomial=3)\n",
    "        # result[f'{col}_savgol_3'] = apply_savgol_filter(feature, window_size=21, polynomial=5)\n",
    "        # result[f'{col}_savgol_4'] = apply_savgol_filter(feature, window_size=11, polynomial=1)\n",
    "        # result[f'{col}_savgol_8'] = apply_savgol_filter(feature, window_size=11, polynomial=2)\n",
    "        # result[f'{col}_savgol_5'] = apply_savgol_filter(feature, window_size=11, polynomial=3)\n",
    "        # result[f'{col}_savgol_6'] = apply_savgol_filter(feature, window_size=11, polynomial=5)\n",
    "        # result[f'{col}_median'] = apply_median_filter(feature)\n",
    "        # result[f'{col}_max'] = apply_maximum_filter(feature, window_size=5)\n",
    "        # result[f'{col}_sav_med'] = apply_median_filter(apply_savgol_filter(feature, window_size=21), window_size=5)\n",
    "        # result[f'{col}_sav_min_max_scale'] = apply_min_max_scale(apply_savgol_filter(feature, window_size=11, polynomial=2))\n",
    "        # result[f'{col}_standard_min_max_scale'] = apply_min_max_scale(apply_standard_scale(feature))\n",
    "        \n",
    "\n",
    "        # result[f'{col}_max'] = apply_max_value(feature)\n",
    "        # result[f'{col}_min'] = apply_min_value(feature)\n",
    "        # result[f'{col}_mean'] = apply_mean(feature)\n",
    "        # result[f'{col}_std'] = apply_std(feature)\n",
    "        # result[f'{col}_is_zero'] = apply_is_zero(feature)\n",
    "        # result[f'{col}_min_max_scale'] = apply_min_max_scale(feature)\n",
    "        result[f'{col}_standard_scale'] = apply_standard_scale(feature)\n",
    "    \n",
    "    # modify features\n",
    "    FEATURE_COLUMNS = result.columns.to_list()\n",
    "    for col in tqdm(FEATURE_COLUMNS):\n",
    "        feature = result[col]\n",
    "        feature = feature.to_numpy()\n",
    "        # result[col] = apply_savgol_filter(feature)\n",
    "        # result[col] = apply_median_filter(feature)\n",
    "        # result[col] = apply_median_filter(apply_savgol_filter(feature, window_size=21), window_size=5)\n",
    "    \n",
    "    # drop default features\n",
    "    result = result.drop(DEFAULT_FEATURE, axis=1)\n",
    "      \n",
    "    return result\n",
    "\n",
    "X_train_1 = feature_engineering(X_train)\n",
    "X_test_1 = feature_engineering(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(scaler_name='MinMaxScaler'):\n",
    "    if scaler_name == 'RobustScaler':\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler_name == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    return scaler\n",
    "# scaler_name = 'RobustScaler'\n",
    "scaler_name = 'MinMaxScaler'\n",
    "# scaler_name = None\n",
    "scaler = get_scaler(scaler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaler:\n",
    "  X_train_scaled = scaler.fit_transform(X_train_1)\n",
    "  X_test_scaled = scaler.transform(X_test_1)\n",
    "else:\n",
    "  X_train_scaled, X_test_scaled = X_train_1.to_numpy(), X_test_1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(scaler, os.path.join('saved_scaler', f'{scaler_name}_wo_max.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_modified, y_train_modified = inverse_mod_X(X_train_scaled), inverse_mod_y(y_train)\n",
    "X_test_modified, y_test_modified = inverse_mod_X(X_test_scaled), inverse_mod_y(y_test)\n",
    "print(X_train_modified.shape, y_train_modified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape, n_output = (X_train_modified.shape[-2], X_train_modified.shape[-1]), y_train_modified.shape[1]\n",
    "def build_model():\n",
    "  # model = create_model_3(input_shape,\n",
    "  #   n_output,\n",
    "  #   head_size=32,\n",
    "  #   num_heads=2,\n",
    "  #   ff_dim=4,\n",
    "  #   num_transformer_blocks=4,\n",
    "  #   mlp_units=[64],\n",
    "  #   dropout=0.25,\n",
    "  #   mlp_dropout=0.25\n",
    "  #   )\n",
    "  # model = create_model_1(input_shape, n_output)\n",
    "  # model = create_model_1_0(input_shape, n_output)\n",
    "  # model = create_model_1_2(input_shape, n_output)\n",
    "  # model = create_model_1_3(input_shape, n_output)\n",
    "  # model = create_model_1_4(input_shape, n_output)\n",
    "  # model = create_model_1_5(input_shape, n_output)\n",
    "  # model = create_model_2(input_shape, n_output)\n",
    "  # model = create_model_2_1(input_shape, n_output)\n",
    "  # model = Sequential([\n",
    "  #     InputLayer(input_shape),\n",
    "  #     Bidirectional(LSTM(64, return_sequences=True)),\n",
    "  #     Bidirectional(LSTM(32, return_sequences=False)),\n",
    "  #     Dense(25, activation= 'selu'),\n",
    "  #     Dense(n_output, activation=\"softmax\")\n",
    "  # ])\n",
    "  model = Sequential([\n",
    "        InputLayer(input_shape),\n",
    "        Conv1D(filters=64,\n",
    "               kernel_size=8,\n",
    "               strides=1,\n",
    "               activation='relu',\n",
    "               padding='same'),\n",
    "        MaxPooling1D(pool_size=4),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(n_output, activation=\"softmax\")\n",
    "    ])\n",
    "  return model\n",
    "tmp = build_model()\n",
    "tmp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=1e-3)\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "                                monitor='val_loss',\n",
    "                                factor=0.5,\n",
    "                                patience=10,\n",
    "                                min_lr=1e-4\n",
    "                              )\n",
    "early_stopping = EarlyStopping(\n",
    "                                monitor='loss',\n",
    "                                min_delta=1e-3,\n",
    "                                patience=12,\n",
    "                                restore_best_weights=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=30, batch_size=32, verbose=0):\n",
    "  model = build_model()\n",
    "  model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=CategoricalCrossentropy(),\n",
    "                metrics=[\n",
    "                    'accuracy',\n",
    "                    tfa.metrics.F1Score(y_train_modified.shape[-1])\n",
    "                ]\n",
    "              )\n",
    "  history = model.fit(X_train_modified, y_train_modified,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            verbose=verbose,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                reduce_lr,\n",
    "                early_stopping,\n",
    "                ]\n",
    "            )\n",
    "  return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, history_list = [], []\n",
    "train_report_list, test_report_list = [], []\n",
    "n_attempt = 5\n",
    "for i in range(n_attempt):\n",
    "    print(\"Attempt:\", i+1)\n",
    "    model, history = train(epochs=150, batch_size=16, verbose=0)\n",
    "    model_list.append(model)\n",
    "    history_list.append(history)\n",
    "    plot_history(history, show_accuracy=False, validate=True)\n",
    "\n",
    "    report_train_metrics = evaluate_model(\n",
    "        model, X_train_modified, y_train_modified)\n",
    "    train_report_list.append(report_train_metrics)\n",
    "    print(\n",
    "        f\"train macro avg f1-score: {report_train_metrics['macro avg']['f1-score']}\")\n",
    "    report_test_metrics = evaluate_model(\n",
    "        model, X_test_modified, y_test_modified)\n",
    "    test_report_list.append(report_test_metrics)\n",
    "    print(\n",
    "        f\"test macro avg f1-score: {report_test_metrics['macro avg']['f1-score']}\")\n",
    "    print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_macro_f1, test_macro_f1 = [], []\n",
    "for e in train_report_list:\n",
    "  train_macro_f1.append(e['macro avg']['f1-score'])\n",
    "for e in test_report_list:\n",
    "  test_macro_f1.append(e['macro avg']['f1-score'])\n",
    "train_macro_f1, test_macro_f1 = np.array(train_macro_f1), np.array(test_macro_f1)\n",
    "\n",
    "print(f\"number of attempt: {n_attempt}\")\n",
    "print(f\"avg macro train: {np.mean(train_macro_f1)}, SD: {np.std(train_macro_f1)}\")\n",
    "print(f\"min: {np.min(train_macro_f1)}, max: {np.max(train_macro_f1)}\")\n",
    "print(f\"avg macro test: {np.mean(test_macro_f1)}, SD: {np.std(test_macro_f1)}\")\n",
    "print(f\"min: {np.min(test_macro_f1)}, max: {np.max(test_macro_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "index = 2\n",
    "model = model_list[index]\n",
    "history = history_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(X_train_modified, batch_size=32)\n",
    "train_predict = np.argmax(train_predict, axis=1)+1\n",
    "train_real = np.argmax(y_train_modified, axis=1)+1\n",
    "\n",
    "# for i in range(len(y_train_modified)):\n",
    "#     print(f\"Index:{i}, Predict:{train_predict[i]}, Real:{train_real[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = f1_score(train_real, train_predict)\n",
    "accuracy_train = accuracy_score(train_real, train_predict)\n",
    "# print(f\"f1: {f1_train:.4f}\\naccuracy: {accuracy_train:.4f}\")\n",
    "print(classification_report(train_real, train_predict, digits=4))\n",
    "print(\"---------------------------------------------------------\")\n",
    "sns.heatmap(confusion_matrix(train_real, train_predict),annot = True,fmt = '2.0f')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_0 = model.predict(X_test_modified)\n",
    "test_predict = np.argmax(test_predict_0, axis=1)+1\n",
    "test_real = np.argmax(y_test_modified, axis=1)+1\n",
    "\n",
    "# for i in range(len(y_test)):\n",
    "#     print(f\"Index:{i}, Predict:{test_real[i]}, Real:{test_real[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(test_real, test_predict)\n",
    "accuracy_test = accuracy_score(test_real, test_predict)\n",
    "# print(f\"f1: {f1_test:.4f}\\naccuracy: {accuracy_test:.4f}\")\n",
    "print(classification_report(test_real, test_predict, digits=4))\n",
    "print(\"---------------------------------------------------------\")\n",
    "sns.heatmap(confusion_matrix(test_real, test_predict),annot = True,fmt = '2.0f')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('py3_10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dae23cc1304adc3b3cd65b9387d8b9dd19248d8974b41ba28d07249eeed262aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
