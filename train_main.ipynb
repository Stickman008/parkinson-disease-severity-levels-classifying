{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, normalize\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "from utils import mod_df, drop_features, inverse_mod_X, inverse_mod_y, apply_savgol_filter, apply_median_filter, apply_maximum_filter, apply_is_zero\n",
    "from my_model import create_model_1, create_model_1_1, create_model_1_2, create_model_1_3, create_model_2, create_model_2_1\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join('data', 'unionTrain.csv'))\n",
    "test_df = pd.read_csv(os.path.join('data', 'unionTest.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mod_df(train_df)\n",
    "X_test, y_test = mod_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "\n",
    "def feature_engineering(df):\n",
    "    result = df.copy()\n",
    "    \n",
    "    # drop features\n",
    "    result = result.drop(['id', 'timestep'], axis=1)\n",
    "    # result = drop_features(result, [15, 16, 17, 18, 20, 21, 23, 24])\n",
    "    \n",
    "    # add features\n",
    "    FEATURE_COLUMNS = result.columns.to_list()\n",
    "    for col in FEATURE_COLUMNS:\n",
    "        feature = result[col]\n",
    "        feature = feature.to_numpy()\n",
    "        result[f'{col}_savgol'] = apply_savgol_filter(feature, window_size=11, polynomial=2)\n",
    "        result[f'{col}_median'] = apply_median_filter(feature)\n",
    "        # result[f'{col}_max'] = apply_maximum_filter(feature)\n",
    "        result[f'{col}_sav_med'] = apply_median_filter(apply_savgol_filter(feature, window_size=21), window_size=5)\n",
    "        result[f'{col}_is_zero'] = apply_is_zero(feature)\n",
    "    \n",
    "    # modify features\n",
    "    FEATURE_COLUMNS = result.columns.to_list()\n",
    "    for col in FEATURE_COLUMNS:\n",
    "        feature = result[col]\n",
    "        feature = feature.to_numpy()\n",
    "        # result[col] = apply_savgol_filter(feature)\n",
    "        # result[col] = apply_median_filter(feature)\n",
    "        # result[col] = apply_maximum_filter(feature)\n",
    "        # result[col] = apply_median_filter(apply_savgol_filter(feature, window_size=21), window_size=5)\n",
    "        \n",
    "     \n",
    "    return result\n",
    "\n",
    "# pool = Pool(4)\n",
    "X_train_1 = feature_engineering(X_train)\n",
    "X_test_1 = feature_engineering(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_1['0X_savgol'][:5], X_train_1['0X'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = X_train_1['0X'].value_counts()\n",
    "# tmp2 = np.array([tmp.index.to_numpy(), tmp.to_numpy()]).T\n",
    "# tmp2[0:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(scaler_name='MinMaxScaler'):\n",
    "    if scaler_name == 'RobustScaler':\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler_name == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "    return scaler\n",
    "# scaler_name = 'RobustScaler'\n",
    "scaler_name = 'MinMaxScaler'\n",
    "scaler = get_scaler(scaler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_1)\n",
    "X_test_scaled = scaler.transform(X_test_1)\n",
    "\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(scaler, os.path.join('saved_scaler', f'{scaler_name}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_modified, y_train_modified = inverse_mod_X(X_train_scaled), inverse_mod_y(y_train)\n",
    "X_test_modified, y_test_modified = inverse_mod_X(X_test_scaled), inverse_mod_y(y_test)\n",
    "print(X_train_modified.shape, y_train_modified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape, n_output = (X_train_modified.shape[-2], X_train_modified.shape[-1]), y_train_modified.shape[1]\n",
    "model = create_model_1(input_shape, n_output)\n",
    "# model = create_model_1_0(input_shape, n_output)\n",
    "# model = create_model_1_1(input_shape, n_output)\n",
    "# model = create_model_1_2(input_shape, n_output)\n",
    "# model = create_model_2(input_shape, n_output)\n",
    "# model = create_model_2_1(input_shape, n_output)\n",
    "# model = Sequential([\n",
    "#         InputLayer(input_shape),\n",
    "#         Bidirectional(LSTM(32, return_sequences=True)),\n",
    "#         Bidirectional(LSTM(32, return_sequences=False)),\n",
    "#         Dense(25, activation='selu'),\n",
    "#         Dense(n_output, activation=\"softmax\")\n",
    "#     ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=CategoricalCrossentropy(),\n",
    "                metrics=[\n",
    "                    'accuracy'\n",
    "                ]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "                                monitor='val_loss',\n",
    "                                factor=0.5,\n",
    "                                patience=4,\n",
    "                                min_lr=1e-4\n",
    "                              )\n",
    "early_stopping = EarlyStopping(\n",
    "                                monitor='loss',\n",
    "                                patience=6\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_modified, y_train_modified,\n",
    "          batch_size=32,\n",
    "          epochs=35,\n",
    "          shuffle=True,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[\n",
    "              reduce_lr,\n",
    "              early_stopping,\n",
    "              ]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model.save(os.path.join(\"saved_models\", f\"{current_time}_{scaler_name}.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(X_train_modified)\n",
    "train_predict = np.argmax(train_predict, axis=1)+1\n",
    "train_real = np.argmax(y_train_modified, axis=1)+1\n",
    "\n",
    "# for i in range(len(y_train_modified)):\n",
    "#     print(f\"Index:{i}, Predict:{train_predict[i]}, Real:{train_real[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = f1_score(train_real, train_predict)\n",
    "accuracy_train = accuracy_score(train_real, train_predict)\n",
    "# print(f\"f1: {f1_train:.4f}\\naccuracy: {accuracy_train:.4f}\")\n",
    "print(classification_report(train_real, train_predict, digits=4))\n",
    "print(\"---------------------------------------------------------\")\n",
    "sns.heatmap(confusion_matrix(train_real, train_predict),annot = True,fmt = '2.0f')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_0 = model.predict(X_test_modified)\n",
    "test_predict = np.argmax(test_predict_0, axis=1)+1\n",
    "test_real = np.argmax(y_test_modified, axis=1)+1\n",
    "\n",
    "# for i in range(len(y_test)):\n",
    "#     print(f\"Index:{i}, Predict:{test_real[i]}, Real:{test_real[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y_test)):\n",
    "    # print(f\"Index:{i}, Predict:{test_predict_0[i]}, Real:{test_real[i]}\")\n",
    "# plt.plot(test_predict==test_real)\n",
    "# plt.plot(np.max(test_predict_0, axis=1))\n",
    "# print(np.max(test_predict_0, axis=1))\n",
    "# plt.plot(test_real==2)\n",
    "# plt.plot(test_predict_0[:, 1]>0.35)\n",
    "# for i in np.arange(0, 1, 0.05):\n",
    "#     print(i, sum((test_predict_0[:, 1]>i)==(test_real==2)))\n",
    "#     plt.plot((test_predict_0[:, 1]>i)==(test_real==2))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(test_real, test_predict)\n",
    "accuracy_test = accuracy_score(test_real, test_predict)\n",
    "# print(f\"f1: {f1_test:.4f}\\naccuracy: {accuracy_test:.4f}\")\n",
    "print(classification_report(test_real, test_predict, digits=4))\n",
    "print(\"---------------------------------------------------------\")\n",
    "sns.heatmap(confusion_matrix(test_real, test_predict),annot = True,fmt = '2.0f')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf2-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "586ea256996c4d1afba0a24bd3ac38219670b30d200ba45ddfed159cb38a21bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
